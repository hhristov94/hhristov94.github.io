---
title:  Multiple Linear Regression
header:
  teaser: /assets/images/linear_combination.png
tags: Vector-Spaces Vectors
categories: Machine-Learning
---

{% include mathjax.html %}

Linear Regression is a simple but effective model that finds a wide range of uses. As the name suggests a linear regression model assumes linearity in its inputs. The main use for it is to model the relationship between a scalar response (or dependent variable) $$y$$ and one or more explanatory variables (or independent variables) $$x_1,x_2,..x_n$$. In the case of one explanatory variable the model is called __Simple Linear Regression__. For more than one explanatory variable, the process is called __Multiple Linear Regression__. 

Theoretically speaking,  the conditional mean of the response variable given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values $$E(Y \vert X) = f(X)$$, where $$X$$ is the dataset in matrix form ( also known as the feature matrix) with $$m$$ rows for the observations and $$n$$ dimensions for the feature columns. Knowing that, we can define a regression problem that looks like this:

$$
y = f(X)+ \epsilon
$$

where $$x\in \mathbb{R}^n$$ are inputs and $$y\in \mathbb{R}$$ are the observed function values.

The model itself has the form

$$
\begin{align}
y & = f(X) + \epsilon\\
  & = 1\theta_0+x_{i1}\theta_1+...+x_{ij}\theta_m+\epsilon,\quad\quad \text{ $i = 1,2,...,m$}\\
  & =X\theta + \epsilon
\end{align}
$$

where the response variable is $$
\begin{pmatrix}y_1 \\ y_2 \\ \vdots\\y_m\end{pmatrix}
$$, the dataset is the matrix $$X =
\begin{pmatrix}
x_1^T \\ x_2^T\\ \vdots \\x_m^T
\end{pmatrix}
= \begin{pmatrix}
1&x_{11}&\cdots & x_{1n}  \\
1&x_{21}&\cdots & x_{2n}\\
\vdots&\vdots&\ddots&\vdots\\
1&x_{m1}&\cdots & x_{mn}
\end{pmatrix}$$, the parameter vector is $$\theta = \begin{pmatrix}\theta_0 \\ \theta_1 \\ \vdots \\\theta_m\end{pmatrix}$$ and the error vector is $$\epsilon = \begin{pmatrix}\epsilon_1 \\\epsilon_2 \\ \vdots\\\epsilon_m\end{pmatrix}$$ .

In order to find the best parameter vector $$\theta$$ that fits closest to the original $$y$$ we need to minimize a loss function. This function would be the *Sum of Squared Errors* (*SSE*) which is often denoted as $$\mathcal{L}$$ and defined as

$$
\begin{align}
\mathcal{L}(\theta) & = \sum_{i=1}^n (y_i - x_i^T\theta)^2 \\
& = (y - X\theta)^T(y - X\theta)\\
& = (y^T-\theta^TX^T)(y-X\theta)
\end{align}
$$

We want to minimize this function and we do so by setting the derivative to zero and algebraicly getting a solution. This method is also known as the Least Squares Solution.

$$
\frac{d\mathcal{L}}{d \theta} = \frac{d}{d\theta}(y^Ty - \theta^TX^Ty - y^TX\theta +\theta^TX^T X\theta) = 0
$$

$$
-X^Ty- X^Ty + 2X^TX\theta = 2X^TX\theta-2X^Ty = 0
$$

$$
2X^TX\theta = 2X^Ty
$$

$$
X^TX\theta = X^Ty
$$

$$
(X^TX)^{-1} X^TX\theta = (X^TX)^{-1} X^Ty
$$

$$
\theta = (X^TX)^{-1} X^Ty
$$

$$
\hat{y}=X\theta=X(X^TX)^{-1}X^Ty
$$

I'll illustrate this in Python.

```python
%matplotlib inline 
# Magics that tells the plots appear inside the notebook
import numpy as np # Linear Algebra module
import matplotlib.pyplot as plt # Data Visualization module
import seaborn as sns
sns.set()
```

After the imports, generate some data to experiment with. The red dots in the two dimentional plots represent the two means of the multivariate gaussians used to generate the data.

```python
num_samples = 100

# The mean values of the three dimensions .
mu = np.array([5, 11, 12])

# The desired covariance matrix.
cov = np.array([
        [1.00, 0.85, 0.95],
        [0.85, 1.00, 0.90],
        [0.95, 0.90, 1.00]
    ])

# Generate the random samples.
df = np.random.multivariate_normal(mu, cov, size=num_samples)

# Plot various projections of the samples.
fig = plt.figure(figsize=(14, 14))

# First subplot
ax = fig.add_subplot(2, 2, 1)
#ax.plot(df[:,0], df[:,1], 'b.',c =(44/255, 160/255, 44/255))
sns.regplot(df[:,0], df[:,1], fit_reg=False)
ax.plot(mu[0], mu[1], 'ro')
plt.xlabel('y[0]')
plt.ylabel('y[1]')
plt.axis('equal')

# Second subplot
ax = fig.add_subplot(2, 2, 2)
#ax.plot(df[:,0], df[:,2], 'b.')
sns.regplot(df[:,0], df[:,2], fit_reg=False)
ax.plot(mu[0], mu[2], 'ro')
plt.xlabel('y[0]')
plt.ylabel('y[2]')
plt.axis('equal')

# Third subplot
ax = fig.add_subplot(2, 2, 3)
#ax.plot(df[:,1], df[:,2], 'b.')
sns.regplot(df[:,1], df[:,2], fit_reg=False)
ax.plot(mu[1], mu[2], 'ro')
plt.xlabel('y[1]')
plt.ylabel('y[2]')
plt.axis('equal')

# Fourth subplot
ax = fig.add_subplot(2, 2, 4, projection= "3d")
ax.scatter(df[:,0], df[:,1], df[:,2])
ax.axis('equal')
plt.show()
```


![png](/assets/images/output_19_0.png)



```python
y = df[:,2]
X = np.delete(df, 2, 1)
```


```python
# create vector of ones...
I = np.ones(shape=y.shape)[..., None]

#...and add to feature matrix
X = np.concatenate((I, X), 1)
print(X[0:5])
```

    [[ 1.          4.58145264 10.98376923]
     [ 1.          5.95738812 12.26031588]
     [ 1.          4.56880514 10.46921756]
     [ 1.          6.3453342  12.53114751]
     [ 1.          5.14757274 10.90459977]]
    


```python
# calculate coefficients using closed-form solution
coeffs = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)
print(coeffs)
```

    [4.58358305 0.60638394 0.39798997]
    


```python
yhat = X.dot(coeffs)
```


```python
# regular grid covering the domain of the data
X = X[:,1:]
mn = np.min(X, axis=0)
mx = np.max(X, axis=0)
X,Y = np.meshgrid(np.linspace(mn[0], mx[0], 20), np.linspace(mn[1], mx[1], 20))
# evaluate it on grid
Z = coeffs[1]*X + coeffs[2]*Y + coeffs[0]
fig = plt.figure()
ax = fig.gca(projection='3d')
ax.plot_surface(X, Y, Z, rstride=1, cstride=1, alpha=0.2)
ax.scatter(df[:,0], df[:,1], df[:,2], s=50)
plt.xlabel('X')
plt.ylabel('Y')
ax.set_zlabel('Z')
ax.axis('equal')
ax.axis('tight')
plt.show()
```


![png](/assets/images/output_24_0.png)



```python
SSE = sum((y-yhat)**2)
print(SSE)
```

    7.334481622361918
    


```python
TSS = sum((y-y.mean())**2)
print(TSS)
```

    91.97595019364105
    


```python
R_Squared = 1.0 - (SSE /TSS)
print(R_Squared)
```

    0.9202565278540715
    


```python
#1-(1-R2)*N-1/N-M-1
Adj_R2 = 1.0 - (1.0 - R_Squared) * 99/97
Adj_R2
```




    0.9186123325520936



## Asumptions

In order for a linear model to work as expected there are a number of assumptions that is adviced to take into account. 


- *Weak exogeneity*. This essentially means that the predictor variables x can be treated as fixed values, rather than random variables. This also implies that the predictor variables are assumed to be error-freeâ€”that is, not contaminated with measurement errors. 
- Linearity. This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables. (Note that $y = \theta_0 + x_1\theta_1+ x_2^2\theta_2$ doesnt violate this assumption even though it has a polynomial)
- Constant variance (a.k.a. homoscedasticity). This means that different values of the response variable have the same variance in their errors, regardless of the values of the predictor variables.*
- Independence of errors. This assumes that the errors of the response variables are uncorrelated with each other. (Actual statistical independence is a stronger condition than mere lack of correlation and is often not needed, although it can be exploited if it is known to hold.) *
